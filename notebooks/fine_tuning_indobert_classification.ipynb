{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-27T16:14:34.886600Z",
     "iopub.status.busy": "2025-09-27T16:14:34.886374Z",
     "iopub.status.idle": "2025-09-27T16:14:36.600610Z",
     "shell.execute_reply": "2025-09-27T16:14:36.599852Z",
     "shell.execute_reply.started": "2025-09-27T16:14:34.886578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:21:50.856239Z",
     "iopub.status.busy": "2025-09-27T16:21:50.855581Z",
     "iopub.status.idle": "2025-09-27T16:23:03.838200Z",
     "shell.execute_reply": "2025-09-27T16:23:03.837304Z",
     "shell.execute_reply.started": "2025-09-27T16:21:50.856212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:23:51.397890Z",
     "iopub.status.busy": "2025-09-27T16:23:51.397566Z",
     "iopub.status.idle": "2025-09-27T16:23:57.377646Z",
     "shell.execute_reply": "2025-09-27T16:23:57.377028Z",
     "shell.execute_reply.started": "2025-09-27T16:23:51.397861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,  \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    hamming_loss\n",
    ")\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Library berhasil diimport!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:24:03.596442Z",
     "iopub.status.busy": "2025-09-27T16:24:03.595706Z",
     "iopub.status.idle": "2025-09-27T16:24:03.665491Z",
     "shell.execute_reply": "2025-09-27T16:24:03.664754Z",
     "shell.execute_reply.started": "2025-09-27T16:24:03.596419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "LABEL_NAMES = [\n",
    "    'Analgesik', \n",
    "    'Antasida', \n",
    "    'Antidiare', \n",
    "    'Antipiretik', \n",
    "    'Antiseptik', \n",
    "    'Dekongestan', \n",
    "    'Ekspektoran', \n",
    "    'Herbal', \n",
    "    'Multivitamin'\n",
    "]\n",
    "NUM_LABELS = len(LABEL_NAMES)\n",
    "print(f\"Jumlah label yang akan digunakan: {NUM_LABELS}\")\n",
    "print(\"Nama Label:\", LABEL_NAMES)\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(LABEL_NAMES)}\n",
    "id2label = {i: label for i, label in enumerate(LABEL_NAMES)}\n",
    "\n",
    "print(\"\\nLabel to ID mapping:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"  '{label}': {idx}\")\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/preprocessed-indobert-train/preprocessed_indobert_data_train_label.csv',\n",
    "                 sep=';'\n",
    "                )\n",
    "df['Jenis_list'] = df['Jenis'].apply(lambda x: [label.strip() for label in x.split(',')])\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=LABEL_NAMES) \n",
    "encoded_labels = mlb.fit_transform(df['Jenis_list'])\n",
    "\n",
    "df['labels_list'] = list(encoded_labels)\n",
    "\n",
    "print(\"\\nContoh data setelah One-Hot Encoding dengan MultiLabelBinarizer:\")\n",
    "print(df[['text', 'Jenis', 'Jenis_list', 'labels_list']].head())\n",
    "print(\"\\nLabel yang dikenali oleh MultiLabelBinarizer:\")\n",
    "print(mlb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEMBAGIAN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:24:10.896090Z",
     "iopub.status.busy": "2025-09-27T16:24:10.895425Z",
     "iopub.status.idle": "2025-09-27T16:24:10.911557Z",
     "shell.execute_reply": "2025-09-27T16:24:10.910761Z",
     "shell.execute_reply.started": "2025-09-27T16:24:10.896058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df['text'].values,\n",
    "    df['labels_list'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=None\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")\n",
    "\n",
    "print(\"\\nPreview Training Data (5 baris):\")\n",
    "print(pd.DataFrame({\"text\": X_train, \"labels\": y_train}).head(5))\n",
    "\n",
    "print(\"\\nPreview Validation Data (3 baris):\")\n",
    "print(pd.DataFrame({\"text\": X_val, \"labels\": y_val}).head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:24:20.982381Z",
     "iopub.status.busy": "2025-09-27T16:24:20.982096Z",
     "iopub.status.idle": "2025-09-27T16:24:20.987925Z",
     "shell.execute_reply": "2025-09-27T16:24:20.987201Z",
     "shell.execute_reply.started": "2025-09-27T16:24:20.982362Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MedicineDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        labels = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:24:32.420112Z",
     "iopub.status.busy": "2025-09-27T16:24:32.419375Z",
     "iopub.status.idle": "2025-09-27T16:24:33.244516Z",
     "shell.execute_reply": "2025-09-27T16:24:33.243924Z",
     "shell.execute_reply.started": "2025-09-27T16:24:32.420085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'indobenchmark/indobert-base-p1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = MedicineDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = MedicineDataset(X_val, y_val, tokenizer)\n",
    "\n",
    "print(\"Dataset berhasil dibuat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:24:36.349559Z",
     "iopub.status.busy": "2025-09-27T16:24:36.349024Z",
     "iopub.status.idle": "2025-09-27T16:24:37.711296Z",
     "shell.execute_reply": "2025-09-27T16:24:37.710518Z",
     "shell.execute_reply.started": "2025-09-27T16:24:36.349535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def to_text_list(seq):\n",
    "    if isinstance(seq, (pd.Series, np.ndarray)):\n",
    "        seq = seq.tolist()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x is None:\n",
    "            out.append(\"\")\n",
    "        elif isinstance(x, float) and math.isnan(x):\n",
    "            out.append(\"\")\n",
    "        elif isinstance(x, (list, tuple)):\n",
    "            out.append(\" \".join(map(str, x)))\n",
    "        else:\n",
    "            out.append(str(x))\n",
    "    return out\n",
    "\n",
    "X_train_txt = to_text_list(X_train)\n",
    "X_val_txt   = to_text_list(X_val)\n",
    "\n",
    "def total_tokens(texts, tok):\n",
    "    enc = tok(texts, truncation=True, padding=False, add_special_tokens=True)\n",
    "    return sum(len(ids) for ids in enc[\"input_ids\"])\n",
    "\n",
    "print(\"Total Tokens:\")\n",
    "print(f\"- Train : {total_tokens(X_train_txt, tokenizer)}\")\n",
    "print(f\"- Val   : {total_tokens(X_val_txt, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:24:46.609238Z",
     "iopub.status.busy": "2025-09-27T16:24:46.608906Z",
     "iopub.status.idle": "2025-09-27T16:24:46.619281Z",
     "shell.execute_reply": "2025-09-27T16:24:46.618525Z",
     "shell.execute_reply.started": "2025-09-27T16:24:46.609216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_text = X_train_txt[0]\n",
    "\n",
    "model_max = getattr(tokenizer, \"model_max_length\", 512)\n",
    "if not isinstance(model_max, int) or model_max > 100000:  \n",
    "    model_max = 512\n",
    "\n",
    "encoded = tokenizer(\n",
    "    sample_text,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=min(len(tokenizer.encode(sample_text, add_special_tokens=True)), model_max),\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"])\n",
    "\n",
    "print(\"[Preview Contoh dari Train Set]\")\n",
    "print(\"Text         :\", (sample_text[:120] + \"…\") if len(sample_text) > 120 else sample_text)\n",
    "print(\"Tokens       :\", tokens[:15])\n",
    "print(\"Input IDs    :\", encoded[\"input_ids\"][:15])\n",
    "print(\"AttentionMask:\", encoded[\"attention_mask\"][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:24:55.812058Z",
     "iopub.status.busy": "2025-09-27T16:24:55.811749Z",
     "iopub.status.idle": "2025-09-27T16:24:55.817102Z",
     "shell.execute_reply": "2025-09-27T16:24:55.816222Z",
     "shell.execute_reply.started": "2025-09-27T16:24:55.812036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"DataLoader berhasil dibuat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:25:03.308221Z",
     "iopub.status.busy": "2025-09-27T16:25:03.307692Z",
     "iopub.status.idle": "2025-09-27T16:25:03.313419Z",
     "shell.execute_reply": "2025-09-27T16:25:03.312508Z",
     "shell.execute_reply.started": "2025-09-27T16:25:03.308195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class IndoBERTMultiLabel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, dropout_rate=0.1):\n",
    "        super(IndoBERTMultiLabel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:25:14.675842Z",
     "iopub.status.busy": "2025-09-27T16:25:14.675558Z",
     "iopub.status.idle": "2025-09-27T16:25:41.623107Z",
     "shell.execute_reply": "2025-09-27T16:25:41.622312Z",
     "shell.execute_reply.started": "2025-09-27T16:25:14.675819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = IndoBERTMultiLabel(MODEL_NAME, NUM_LABELS)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 25\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "num_warmup_steps = int(0.10 * num_training_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "print(f\"Model berhasil diinisialisasi!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINE-TUNING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:26:08.350167Z",
     "iopub.status.busy": "2025-09-27T16:26:08.349550Z",
     "iopub.status.idle": "2025-09-27T16:26:08.359105Z",
     "shell.execute_reply": "2025-09-27T16:26:08.357923Z",
     "shell.execute_reply.started": "2025-09-27T16:26:08.350143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions = torch.sigmoid(logits)\n",
    "            predictions = (predictions > threshold).float()\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    f1_micro = f1_score(all_labels, all_predictions, average='micro')\n",
    "    precision_micro = precision_score(all_labels, all_predictions, average='micro')\n",
    "    recall_micro = recall_score(all_labels, all_predictions, average='micro')\n",
    "    hamming = hamming_loss(all_labels, all_predictions)\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'f1_micro': f1_micro,\n",
    "        'precision_micro': precision_micro,\n",
    "        'recall_micro': recall_micro,\n",
    "        'hamming_loss': hamming\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING & PERFORMANCE EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:26:25.829882Z",
     "iopub.status.busy": "2025-09-27T16:26:25.829595Z",
     "iopub.status.idle": "2025-09-27T17:39:10.999125Z",
     "shell.execute_reply": "2025-09-27T17:39:10.998334Z",
     "shell.execute_reply.started": "2025-09-27T16:26:25.829853Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Memulai training...\")\n",
    "\n",
    "patience = 3  \n",
    "epochs_no_improve = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "best_val_precision = 0.0\n",
    "best_val_recall = 0.0\n",
    "best_val_hamming = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, criterion, device)\n",
    "    train_loss_history.append(train_loss)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "    val_loss_history.append(val_metrics['loss'])\n",
    "    print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"Val F1 (micro): {val_metrics['f1_micro']:.4f}\")\n",
    "    print(f\"Val Precision (micro): {val_metrics['precision_micro']:.4f}\")\n",
    "    print(f\"Val Recall (micro): {val_metrics['recall_micro']:.4f}\")\n",
    "    print(f\"Val Hamming Loss: {val_metrics['hamming_loss']:.4f}\")\n",
    "    \n",
    "    if val_metrics['loss'] < best_val_loss:\n",
    "        best_val_loss = val_metrics['loss']\n",
    "        best_val_f1 = val_metrics['f1_micro']\n",
    "        best_val_precision = val_metrics['precision_micro']\n",
    "        best_val_recall = val_metrics['recall_micro']\n",
    "        best_val_hamming = val_metrics['hamming_loss']\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        epochs_no_improve = 0\n",
    "        print(f\"Validation loss membaik! Menyimpan model...\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Validation loss tidak membaik. Counter: {epochs_no_improve}/{patience}\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"\\nEarly stopping dipicu pada epoch {epoch + 1}!\")\n",
    "        break\n",
    "\n",
    "print(\"\\nTraining selesai!\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Hasil Terbaik pada Data Validasi:\")\n",
    "print(f\"  -> F1-Score (micro): {best_val_f1:.4f}\")\n",
    "print(f\"  -> Precision (micro): {best_val_precision:.4f}\")\n",
    "print(f\"  -> Recall (micro)   : {best_val_recall:.4f}\")\n",
    "print(f\"  -> Hamming Loss: {best_val_hamming:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if best_model_state is None:\n",
    "    best_model_state = model.state_dict().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:45:05.423397Z",
     "iopub.status.busy": "2025-09-27T17:45:05.422554Z",
     "iopub.status.idle": "2025-09-27T17:45:05.721470Z",
     "shell.execute_reply": "2025-09-27T17:45:05.720859Z",
     "shell.execute_reply.started": "2025-09-27T17:45:05.423372Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.title('Training vs Validation Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:59:30.767749Z",
     "iopub.status.busy": "2025-09-27T17:59:30.767469Z",
     "iopub.status.idle": "2025-09-27T17:59:58.337531Z",
     "shell.execute_reply": "2025-09-27T17:59:58.336899Z",
     "shell.execute_reply.started": "2025-09-27T17:59:30.767729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "def get_detailed_metrics_validation(model, data_loader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            predictions = torch.sigmoid(logits)\n",
    "            predictions = (predictions > threshold).float()\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_labels), np.array(all_predictions)\n",
    "\n",
    "y_true_val, y_pred_val = get_detailed_metrics_validation(model, val_loader, device)\n",
    "\n",
    "print(\"\\nMetrik F1-Score per Kelas (Data Validasi - Model Terbaik):\")\n",
    "print(\"=\" * 65)\n",
    "for i, label_name in enumerate(LABEL_NAMES):\n",
    "    f1 = f1_score(y_true_val[:, i], y_pred_val[:, i], zero_division=0)\n",
    "    precision = precision_score(y_true_val[:, i], y_pred_val[:, i], zero_division=0)\n",
    "    recall = recall_score(y_true_val[:, i], y_pred_val[:, i], zero_division=0)\n",
    "    hamming = hamming_loss(y_true_val[:, i], y_pred_val[:, i])\n",
    "    \n",
    "    print(f\"{label_name:15} - F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, Hamming: {hamming:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_save_path = \"/kaggle/working/indobert_medicine_classifier_fixed\"\n",
    "model.eval()\n",
    "current_label_mapping = {\n",
    "    'label_names': LABEL_NAMES,\n",
    "    'label2id': label2id,\n",
    "    'id2label': id2label\n",
    "}\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'num_labels': NUM_LABELS,\n",
    "        'label_mapping': current_label_mapping\n",
    "    }\n",
    "}, f\"{model_save_path}_model.pth\")\n",
    "\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model berhasil disimpan di: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_medicine_labels(text, model, tokenizer, device, label_names, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Fungsi untuk memprediksi label obat dari teks baru\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        predictions = (probabilities > threshold).float()\n",
    "    \n",
    "    # Convert to readable format\n",
    "    results = []\n",
    "    for i, (prob, pred) in enumerate(zip(probabilities[0], predictions[0])):\n",
    "        results.append({\n",
    "            'label': label_names[i],\n",
    "            'probability': prob.item(),\n",
    "            'predicted': bool(pred.item())\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "sample_text = \"promag obat bahan alami dengan ekstrak kayu manis untuk asam lambung\"\n",
    "predictions = predict_medicine_labels(sample_text, model, tokenizer, device, LABEL_NAMES)\n",
    "\n",
    "print(f\"\\nContoh prediksi untuk: '{sample_text}'\")\n",
    "print(\"-\" * 60)\n",
    "for result in predictions:\n",
    "    if result['predicted']:\n",
    "        print(f\"✓ {result['label']}: {result['probability']:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {result['label']}: {result['probability']:.4f}\")\n",
    "\n",
    "print(\"\\nScript selesai! Model berhasil di-train dan siap digunakan.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8316176,
     "sourceId": 13137516,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8353817,
     "sourceId": 13450963,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
